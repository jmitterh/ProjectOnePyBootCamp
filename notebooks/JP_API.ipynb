{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import pprint as pprint\n",
    "from pandas.io.json import json_normalize\n",
    "import gmaps\n",
    "import gmaps.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file to be saved and pulled from\n",
    "rawData = \"../data/rawData/jp_normalized_raw_dataframe.csv\"\n",
    "# renamed data file to be saved and pulled from\n",
    "orgRawData = \"../data/rawData/jp_organized_raw_dataframe.csv\"\n",
    "# No missing data file to be saved and pulled from\n",
    "noMisingData = \"../data/rawData/jp_no_missing_raw_data.csv\"\n",
    "# Raw geo data file to be saved and pulled from\n",
    "rawGeoData = \"../data/rawData/jp_geo_raw_dataframe.csv\"\n",
    "\n",
    "# path csv file location\n",
    "pathRawData = \"../data/rawData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to update the config file with your API key\n",
    "from config import api_key\n",
    "from config import api_id\n",
    "from config import gkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.adzuna.com/v1/api/jobs/us/search/\"\n",
    "#have to create iteration to get all pages of data\n",
    "page = \"1\"\n",
    "api_details =\"?\" + \"app_id=\" + api_id + \"&app_key=\" + api_key\n",
    "# Build query URL\n",
    "query_url = url + page + api_details\n",
    "query_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_response = requests.get(query_url)\n",
    "data_json = data_response.json()\n",
    "type(data_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data using json_normalize\n",
    "#LINK: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.json_normalize.html\n",
    "\n",
    "pages_test = [str(x) for x in range(1,101)]\n",
    "\n",
    "counter = 0\n",
    "# Build query URL\n",
    "data_pages_df = pd.DataFrame()\n",
    "\n",
    "for page_num in pages_test:\n",
    "    response = requests.get(url + page_num + api_details).json()\n",
    "    \n",
    "    #create a df to store the normalized page that has the job postings\n",
    "    normalized_page = json_normalize(response['results'])\n",
    "    \n",
    "    #add normalized data into a df\n",
    "    data_page_df = pd.DataFrame(normalized_page)\n",
    "    \n",
    "    #add the url column to df incase an error occurs you know what page you were on\n",
    "    data_page_df['query_url'] = 'page number ' + page_num\n",
    "    \n",
    "    #append to a new df so each page can be saved\n",
    "    #Sorting because non-concatenation axis is not aligned\n",
    "    data_pages_df = data_pages_df.append(data_page_df, sort=True)\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"Page {page_num} iteration complete\")\n",
    "    \n",
    "print(f'total rows iterated | {counter * 10}')   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save raw data to dataRaw folder\n",
    "data_pages_df.to_csv(rawData, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the csv raw data\n",
    "csv_file = pd.read_csv(rawData)\n",
    "csv_file.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#making a copy\n",
    "csv_file_copy = csv_file.copy()\n",
    "\n",
    "#getting only the rows that we are using\n",
    "csv_file_copy = csv_file_copy[[\"id\",\"title\",\"category.label\",\"company.display_name\", \"location.display_name\", \"latitude\",\"longitude\" ]]\n",
    "csv_file_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the column names\n",
    "\n",
    "renamed_csv_file = csv_file_copy.rename(columns = {\n",
    "    \"id\": \"Job Posting ID\",\n",
    "    \"title\": \"Job Title\",\n",
    "    \"category.label\":\"Category\",\n",
    "    \"company.display_name\":\"Company Name\",\n",
    "    \"location.display_name\":\"Location\",\n",
    "    \"latitude\":\"Lat\",\n",
    "    \"longitude\":\"Lng\"\n",
    "})\n",
    "renamed_csv_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save raw data to dataRaw folder\n",
    "renamed_csv_file.to_csv(orgRawData, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the csv org data\n",
    "org_raw_csv_file = pd.read_csv(orgRawData)\n",
    "org_raw_csv_file_copy = org_raw_csv_file.copy()\n",
    "org_raw_csv_file_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Rows of all columns\n",
    "org_raw_csv_file_copy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Rows with missing data from lat and lng\n",
    "org_data = org_raw_csv_file_copy.dropna()\n",
    "org_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save organized raw data into rawData folder\n",
    "org_data.to_csv(noMisingData, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the csv org data\n",
    "no_missing_data_csv_file = pd.read_csv(noMisingData)\n",
    "no_missing_data = no_missing_data_csv_file.copy()\n",
    "no_missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new column for combined LatLng\n",
    "no_missing_data['LatLng'] = no_missing_data['Lat'].map(str) + \",\" + no_missing_data['Lng'].map(str)\n",
    "\n",
    "LatLng = list(no_missing_data['LatLng'])\n",
    "LatLng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new columns for the new data pulling from google api\n",
    "no_missing_data['geocode_data'] = ''\n",
    "no_missing_data['city'] = ''\n",
    "no_missing_data['state'] = ''\n",
    "no_missing_data['country'] = ''\n",
    "\n",
    "#Create a function to call each url with the datas Lat Long\n",
    "def reverse_geocode(latlng):\n",
    "    #this going to store the url for each LatLng\n",
    "    result = {}\n",
    "    geocode_url = \"https://maps.googleapis.com/maps/api/geocode/json?latlng=\"\n",
    "    api_geo_details =\"&key=\" + gkey\n",
    "    query_geo_url = geocode_url + latlng + api_geo_details\n",
    "    geo_data_response = requests.get(query_geo_url)\n",
    "    geo_data_json = geo_data_response.json()\n",
    "    \n",
    "    if len(geo_data_json['results']) > 0:\n",
    "        result = geo_data_json['results'][0]\n",
    "    return result\n",
    "\n",
    "#use map to call the function for all column LatLng\n",
    "no_missing_data['geocode_data'] = no_missing_data['LatLng'].map(reverse_geocode)\n",
    "no_missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function that searches for Country\n",
    "def parse_country(geocode_data):\n",
    "    if (not geocode_data is None) and ('address_components' in geocode_data):\n",
    "        for component in geocode_data['address_components']:\n",
    "            if 'country' in component['types']:\n",
    "                return component['long_name']\n",
    "    return None\n",
    "\n",
    "#Created a function that searches for state\n",
    "def parse_state(geocode_data):\n",
    "    if (not geocode_data is None) and ('address_components' in geocode_data):\n",
    "        for component in geocode_data['address_components']:\n",
    "            if 'administrative_area_level_1' in component['types']:\n",
    "                return component['long_name']\n",
    "    return None\n",
    "\n",
    "#Created a function that searches for city/town \n",
    "def parse_city(geocode_data):\n",
    "    if (not geocode_data is None) and ('address_components' in geocode_data):\n",
    "        for component in geocode_data['address_components']:\n",
    "            if 'locality' in component['types']:\n",
    "                return component['long_name']\n",
    "            elif 'postal_town' in component['types']:\n",
    "                return component['long_name']\n",
    "            elif 'administrative_area_level_2' in component['types']:\n",
    "                return component['long_name']\n",
    "            elif 'administrative_area_level_1' in component['types']:\n",
    "                return component['long_name']\n",
    "    return None\n",
    "\n",
    "#append these functions for city state and country using the maps function\n",
    "no_missing_data['city'] = no_missing_data['geocode_data'].map(parse_city)\n",
    "no_missing_data['state'] = no_missing_data['geocode_data'].map(parse_state)\n",
    "no_missing_data['country'] = no_missing_data['geocode_data'].map(parse_country)\n",
    "no_missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save raw data to dataRaw folder\n",
    "geo_raw_df = no_missing_data\n",
    "geo_raw_df.to_csv(rawGeoData, index=False)\n",
    "geo_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
